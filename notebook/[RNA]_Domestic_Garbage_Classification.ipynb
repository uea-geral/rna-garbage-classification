{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Atividade 3: Classifica√ß√£o de Lixo Dom√©stico\n",
        "\n",
        "> Classifica√ß√£o de lixo dom√©stico utilizando Python e Keras.\n",
        "\n",
        "## Desafio\n",
        "\n",
        "Classificar alguns objetos encontrados em lixo dom√©stico usando o _dataset_ do Kaggle dispon√≠vel em https://www.kaggle.com/datasets/farzadnekouei/trash-type-image-dataset/.\n",
        "O conjunto de dados possui 6 classes (6 tipos de lixo):\n",
        "\n",
        "- üì¶ Caixas de papel√£o;\n",
        "- ü•Ç Vidro;\n",
        "- üõ¢Ô∏è Metal;\n",
        "- üóûÔ∏è Papel;\n",
        "- ü•§ Pl√°stico;\n",
        "- üóëÔ∏è Entulhos (restos de embalagem, comida e outros que n√£o se enquadram nas categorias anteriores).\n",
        "\n",
        "## Autores\n",
        "\n",
        "- Orientadora: Ello√° B. Guedes - [@elloa](https://github.com/elloa)\n",
        "- Time:\n",
        "  - Debora Souza Barros - [@Debby-Barros](https://github.com/Debby-Barros)\n",
        "  - Diana Martins - [@ddianaom](https://github.com/ddianaom)\n",
        "  - Gabriel Dos Santos Lima - [@gabrielSantosLima](https://github.com/gabrielSantosLima)\n",
        "  - Thiago Marques - [@tmmarquess ](https://github.com/tmmarquess)\n"
      ],
      "metadata": {
        "id": "aG9G_aBWV5pG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Etapa 0: Configura√ß√£o do ambiente\n",
        "\n",
        "Os t√≥picos que ser√£o abordados nesta etapa:\n",
        "* Importa√ß√£o das bibliotecas\n",
        "* Baixar o _dataset_ para o arquivo local do projeto  "
      ],
      "metadata": {
        "id": "m9M6pdmX10Aw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install optuna keras_tuner tensorflow[and-cuda] kaggle"
      ],
      "metadata": {
        "id": "8VS_fKZfIgCp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import zipfile\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "\n",
        "os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n",
        "\n",
        "import keras\n",
        "import keras_tuner as kt\n",
        "\n",
        "from collections import Counter\n",
        "from glob import glob\n",
        "from keras_tuner import HyperModel\n",
        "from keras.utils import to_categorical\n",
        "from keras.callbacks import EarlyStopping\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix"
      ],
      "metadata": {
        "id": "p_xBNNMN2VgZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# baixando do kaggle\n",
        "if not os.path.isdir('dataset'):\n",
        "  !rm -r sample_data\n",
        "  !kaggle datasets download -d farzadnekouei/trash-type-image-dataset\n",
        "  !unzip trash-type-image-dataset.zip\n",
        "  !rm trash-type-image-dataset.zip\n",
        "  !mv TrashType_Image_Dataset dataset\n",
        "else:\n",
        "  print(\"Conjunto de dados j√° existe no diret√≥rio atual.\")"
      ],
      "metadata": {
        "id": "FANF1R_RD5ku"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Etapa 1: Importa√ß√£o do conjunto de dados\n",
        "\n",
        "Os t√≥picos que ser√£o abordados nesta etapa:\n",
        "* Importar o _dataset_\n",
        "* Verificar quantos exemplos o _dataset_ possui"
      ],
      "metadata": {
        "id": "r7aLLjq516aJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# diret√≥rio do dataset\n",
        "base_dir = 'dataset'\n",
        "\n",
        "# quantidade de exemplos do dataset\n",
        "image_files = glob(os.path.join(base_dir, '**', '*.jpg'), recursive=True)\n",
        "print(f'O dataset possui {len(image_files)} imagens')"
      ],
      "metadata": {
        "id": "SyRhZ-kAfCyb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Etapa 2: An√°lise explorat√≥ria\n",
        "\n",
        "Os t√≥picos que podem ser abordados nesta etapa:\n",
        "* Buscar explorar informa√ß√µes relevantes sobre a base de dados. Algumas sugest√µes de perguntas que podem servir como ponto de partida:\n",
        "  * Quantas classes existem?\n",
        "  * Quantos exemplos cada classe possui?\n",
        "* Analisar a qualidade das imagens do _dataset_ e descrever as limita√ß√µes que podem ser encontradas (se poss√≠vel apresentar exemplos)"
      ],
      "metadata": {
        "id": "RE_VfuHl1-_3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# quantidade de classes no dataset\n",
        "count_classes = 0\n",
        "for dir in os.listdir(base_dir):\n",
        "  count_classes += 1\n",
        "\n",
        "print(f\"No dataset 'Trash type' existem {count_classes} classes\")"
      ],
      "metadata": {
        "id": "LAPrpBU83oZp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# quantidades de exemplos em cada classe\n",
        "files_count = {}\n",
        "for root, dirs, files in os.walk(base_dir):\n",
        "  for dir in dirs:\n",
        "    qtd_files = os.path.join(root, dir)\n",
        "    count = len(os.listdir(qtd_files))\n",
        "    files_count[dir] = count\n",
        "\n",
        "\n",
        "for key, item in files_count.items():\n",
        "  print(f'Na classe \"{key}\" existem {item} imagens')"
      ],
      "metadata": {
        "id": "LYNnFxJqnNg9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dimens√µes das imagens\n",
        "def img_dimensions(img_dir):\n",
        "    files = os.listdir(img_dir)\n",
        "    dim = []\n",
        "    for file in files:\n",
        "        img_path = os.path.join(img_dir, file)\n",
        "        img = cv2.imread(img_path)\n",
        "\n",
        "        height, width, channels = img.shape\n",
        "        dim.append((height, width))\n",
        "\n",
        "    count_dim = Counter(dim)\n",
        "\n",
        "    print(\"Dimens√µes mais comuns:\")\n",
        "    for dim, freq in count_dim.most_common(15):\n",
        "        print(f\"Dimens√£o (altura x largura): {dim}, Frequ√™ncia: {freq}\")\n",
        "    print('\\n')\n",
        "\n",
        "\n",
        "for dir in os.listdir(base_dir):\n",
        "  dir_path = os.path.join(base_dir, dir)\n",
        "  if os.path.isdir(dir_path):\n",
        "    print(f'Analisando imagens em: {dir_path}')\n",
        "    img_dimensions(dir_path)"
      ],
      "metadata": {
        "id": "csk8glW9sY4P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# plotando algumas imagens das classes do dataset 'Trash Type'\n",
        "def plot_images_from_subfolders(base_dir, num_images=3):\n",
        "    subfolders = [os.path.join(base_dir, folder) for folder in os.listdir(base_dir) if os.path.isdir(os.path.join(base_dir, folder))]\n",
        "\n",
        "    for folder_path in subfolders:\n",
        "        print(f\"Imagens de: {folder_path}\")\n",
        "        fig, axes = plt.subplots(nrows=1, ncols=num_images, figsize=(15, 5))\n",
        "        files = os.listdir(folder_path)\n",
        "\n",
        "        for i in range(num_images):\n",
        "            img_path = os.path.join(folder_path, files[i])\n",
        "            img = cv2.imread(img_path)\n",
        "            img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "            axes[i].imshow(img_rgb)\n",
        "            axes[i].axis('off')\n",
        "        plt.show()\n",
        "\n",
        "plot_images_from_subfolders(base_dir, 3)"
      ],
      "metadata": {
        "id": "f8VummcjylOw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Etapa 3: Pr√©-processamento\n",
        "\n",
        "Os t√≥picos que podem ser abordados nesta etapa:\n",
        "* Definir o tamanho da grade de busca a ser contemplada\n",
        "* Preparar o conjunto de dados para o treinamento com a estrat√©gia de valida√ß√£o cruzada _holdout_"
      ],
      "metadata": {
        "id": "rAuldwiY2FA4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Utilizano Keras\n",
        "param_grid_keras = {\n",
        "    'units': [32, 64, 128, 256],\n",
        "    'activation': ['relu', 'tanh', 'sigmoid'],\n",
        "    'optimizer': ['adam', 'sgd', 'rmsprop'],\n",
        "    'learning_rate': [0.01, 0.1, 0.4],\n",
        "    'batch_size': [32, 64],\n",
        "    'epochs': [10, 20, 30]\n",
        "}"
      ],
      "metadata": {
        "id": "EbD3vXUBF3cR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Preparar o conjunto de dados para o treinamento com a estrat√©gia de valida√ß√£o cruzada holdout\n",
        "data = []\n",
        "labels = []\n",
        "\n",
        "for root, dirs, files in os.walk(base_dir):\n",
        "  for dir in dirs:\n",
        "    for file in os.listdir(os.path.join(root, dir)):\n",
        "      img_path = os.path.join(root, dir, file)\n",
        "      img = cv2.imread(img_path)\n",
        "      img = np.array(img)\n",
        "      label = dir\n",
        "      data.append(img)\n",
        "      labels.append(label)"
      ],
      "metadata": {
        "id": "mJV5NI4Fz3Sq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Organizando algumas informa√ß√µes sobre o conjunto de dados\n",
        "num_classes = 6\n",
        "image_shape = data[0].shape"
      ],
      "metadata": {
        "id": "yP_OpDYk5WNl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Preparando a transforma√ß√£o dos r√≥tulos para atributos categ√≥ricos utilizando OneHotEncoder\n",
        "encoder = LabelEncoder()"
      ],
      "metadata": {
        "id": "xUgTrGNmuPNJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = np.array(data)\n",
        "y = to_categorical(encoder.fit_transform(np.array(labels)))\n",
        "\n",
        "x_train_temp, x_test, y_train_temp, y_test = train_test_split(X, y, test_size=.3, shuffle=True) # Holdout 70/30\n",
        "x_train, x_val, y_train, y_val = train_test_split(x_train_temp, y_train_temp, test_size=.2, shuffle=True) # Holdout 80/20"
      ],
      "metadata": {
        "id": "Hs7dCWJc7yaQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Etapa 4: Treinamento e testes dos modelos"
      ],
      "metadata": {
        "id": "IJpxz5pw2XVX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Os t√≥picos que ser√£o abordados nesta etapa:\n",
        "* Definir qual o modelo que ser√° utilizado e quais arquiteturas ser√£o avaliadas\n",
        "* Preparar modelo(s) para grade de busca\n",
        "* Treinamento\n",
        "* Teste do(s) modelo(s)"
      ],
      "metadata": {
        "id": "VOgZtHjg5icQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CNNModel(HyperModel):\n",
        "    def __init__(self, input_shape, num_classes, name=None, tunable=True):\n",
        "       super().__init__(name, tunable)\n",
        "       self.input_shape = input_shape\n",
        "       self.num_classes = num_classes\n",
        "\n",
        "    def __choice_param(self, param, hp):\n",
        "      return hp.Choice(param, param_grid_keras[param])\n",
        "\n",
        "    def build(self, hp):\n",
        "        model = keras.models.Sequential()\n",
        "\n",
        "        # Input Layer\n",
        "        model.add(keras.layers.Input(shape=self.input_shape))\n",
        "\n",
        "        # Feature Layers\n",
        "        model.add(keras.layers.Conv2D(64, kernel_size=(3, 3), activation=self.__choice_param('activation', hp)))\n",
        "        model.add(keras.layers.Conv2D(64, kernel_size=(3, 3), activation=self.__choice_param('activation', hp)))\n",
        "        model.add(keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
        "        model.add(keras.layers.Conv2D(128, kernel_size=(3, 3), activation=self.__choice_param('activation', hp)))\n",
        "        model.add(keras.layers.Conv2D(128, kernel_size=(3, 3), activation=self.__choice_param('activation', hp)))\n",
        "        model.add(keras.layers.GlobalAveragePooling2D())\n",
        "        model.add(keras.layers.Dropout(0.5))\n",
        "\n",
        "        # Dense Layers\n",
        "        model.add(keras.layers.Dense(128, activation=self.__choice_param('activation', hp)))\n",
        "        model.add(keras.layers.Dense(self.num_classes, activation='softmax'))\n",
        "\n",
        "        # Preparing model to train\n",
        "        model.compile(loss = 'categorical_crossentropy',\n",
        "                      optimizer=keras.optimizers.Adam(learning_rate=self.__choice_param('learning_rate', hp)),\n",
        "                      metrics=['accuracy'])\n",
        "        return model\n",
        "\n",
        "    def fit(self, hp, model, *args, **kwargs):\n",
        "        return model.fit(\n",
        "            *args,\n",
        "            batch_size=self.__choice_param('batch_size', hp),\n",
        "            **kwargs,\n",
        "        )"
      ],
      "metadata": {
        "id": "6XjjfVSwGfYd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Treinando"
      ],
      "metadata": {
        "id": "x7mYcnjm5w_S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 1\n",
        "max_trials = 5\n",
        "early_stopping_callback = EarlyStopping(monitor='val_loss',\n",
        "                                        patience=3)"
      ],
      "metadata": {
        "id": "OPL6Sw425y8R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Treinamento: Rede customizada"
      ],
      "metadata": {
        "id": "PanaCeTwB_J5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tuner_custom = kt.RandomSearch(\n",
        "    CNNModel(image_shape, num_classes),\n",
        "    objective='val_accuracy',\n",
        "    directory='models/custom',\n",
        "    overwrite=True,\n",
        "    max_trials=max_trials)"
      ],
      "metadata": {
        "id": "s1nPP2MUgX2g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tuner_custom.search(\n",
        "    x_train,\n",
        "    y_train,\n",
        "    epochs=epochs,\n",
        "    validation_data=(x_val, y_val),\n",
        "    callbacks=[early_stopping_callback])"
      ],
      "metadata": {
        "id": "YBlAymT1k3Wf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Treinamento: ResNet"
      ],
      "metadata": {
        "id": "btltATbVCGDh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 32"
      ],
      "metadata": {
        "id": "lQTAuSj_SCkc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tuner_resnet = kt.RandomSearch(\n",
        "    kt.applications.HyperResNet(input_shape=image_shape, classes=num_classes),\n",
        "    objective='val_accuracy',\n",
        "    directory='models/resnet',\n",
        "    overwrite=True,\n",
        "    max_trials=max_trials)"
      ],
      "metadata": {
        "id": "als2kBR1g2jO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tuner_resnet.search(\n",
        "    x_train,\n",
        "    y_train,\n",
        "    epochs=epochs,\n",
        "    batch_size=batch_size,\n",
        "    validation_data=(x_val, y_val),\n",
        "    callbacks=[early_stopping_callback])"
      ],
      "metadata": {
        "id": "0AOVF4imk14m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Recuperando os melhores modelos"
      ],
      "metadata": {
        "id": "yQtrconkmijB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_best_model(tuner):\n",
        "  best_models = tuner.get_best_models(num_models=1)\n",
        "  return best_models[0]"
      ],
      "metadata": {
        "id": "qQQKYUgCltV_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "custom_best_model = get_best_model(tuner_custom)\n",
        "custom_best_model.summary()"
      ],
      "metadata": {
        "id": "cE2ZQaoTmprH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "resnet_best_model = get_best_model(tuner_resnet)\n",
        "resnet_best_model.summary()"
      ],
      "metadata": {
        "id": "uEGPm7JjmrZW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def print_history_of_model(model):\n",
        "  history = model.fit(\n",
        "      x_train,\n",
        "      y_train,\n",
        "      batch_size=batch_size,\n",
        "      epochs=epochs,\n",
        "      validation_data=(x_val, y_val))\n",
        "\n",
        "  plt.figure(figsize=(6,6))\n",
        "  plt.plot(history.history['accuracy'], label='acur√°cia do treinamento')\n",
        "  plt.plot(history.history['val_accuracy'], label='acur√°cia da valida√ß√£o')\n",
        "  plt.title('Hist√≥rico de Acur√°cia')\n",
        "  plt.xlabel('√âpocas')\n",
        "  plt.ylabel('Acur√°cia')\n",
        "  plt.legend()\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "Objl-Pi1_oEs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print_history_of_model(custom_best_model)"
      ],
      "metadata": {
        "id": "ori2xI3jAgOp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print_history_of_model(resnet_best_model)"
      ],
      "metadata": {
        "id": "tlITJNf-AjTv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Salvando os modelos"
      ],
      "metadata": {
        "id": "QqBKQzVTjFIe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "custom_best_model.save('model_custom.keras')"
      ],
      "metadata": {
        "id": "VeOOasEBjEEn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "resnet_best_model.save('model_resnet.keras')"
      ],
      "metadata": {
        "id": "ZeKyT9GZmC1Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Etapa 5: An√°lise quantitativa e qualitativa de desempenho dos modelos avaliados\n",
        "\n",
        "Os t√≥picos que podem ser abordados nesta etapa:\n",
        "* An√°lise quantitativa do(s) modelo(s)\n",
        "* An√°lise qualitativa do(s) modelo(s)\n",
        "* Conclus√£o. Incluir na disserta√ß√£o:\n",
        "  * Sugest√µes de melhoria;\n",
        "  * Desafios;\n",
        "  * Pr√≥ximos passos."
      ],
      "metadata": {
        "id": "egjygsgK2bo_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Avaliando a qualidade dos modelos"
      ],
      "metadata": {
        "id": "b5dWU8cxktK-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Fun√ß√£o de avalia√ß√£o\n",
        "def show_metrics(y_true, y_pred):\n",
        "    # Matriz de Confus√£o\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    plt.title('Matriz de Confus√£o')\n",
        "    sns.heatmap(cm, annot=True, fmt='.0f', cmap='Blues')\n",
        "    plt.show()\n",
        "\n",
        "    # Acur√°cia\n",
        "    acc = accuracy_score(y_true, y_pred)\n",
        "    print(f\"\\nAcur√°cia: {acc:.4f}\")\n",
        "\n",
        "    # F1-Score\n",
        "    f_score = f1_score(y_true, y_pred, average='weighted')\n",
        "    print(f\"F1-Score: {f_score:.4f}\")\n",
        "\n",
        "    # Precis√£o\n",
        "    precision = precision_score(y_true, y_pred, average='weighted')\n",
        "    print(f\"Precis√£o: {precision:.4f}\")\n",
        "\n",
        "    # Revoca√ß√£o\n",
        "    recall = recall_score(y_true, y_pred, average='weighted')\n",
        "    print(f\"Revoca√ß√£o: {recall:.4f}\")\n",
        "\n",
        "def load_and_predict(model_path, x_test):\n",
        "    model = keras.models.load_model(model_path)\n",
        "    y_pred = model.predict(x_test)\n",
        "    return np.argmax(y_pred, axis=1)\n",
        "\n",
        "def evaluate_model(model_name, y_test_classes, y_pred):\n",
        "    print(f\"M√©tricas do {model_name}:\")\n",
        "    show_metrics(y_test_classes, y_pred)"
      ],
      "metadata": {
        "id": "Kfy75_u6kwxX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Carregar e prever usando os modelos\n",
        "y_pred_custom = load_and_predict('model_custom.keras', x_test)\n",
        "y_pred_resnet = load_and_predict('model_resnet.keras', x_test)\n",
        "\n",
        "# Converter y_test para classes\n",
        "y_test_classes = np.argmax(y_test, axis=1)"
      ],
      "metadata": {
        "id": "9oTeoRPA2l7Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate_model(\"Modelo Custom\", y_test_classes, y_pred_custom)"
      ],
      "metadata": {
        "id": "YzeDVz-wQV4s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate_model(\"Modelo ResNet\", y_test_classes, y_pred_resnet)"
      ],
      "metadata": {
        "id": "23Ps_eZqQW-A"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}